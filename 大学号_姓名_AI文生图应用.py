import os
import json
import gradio as gr
from openai import OpenAI

# 1. é…ç½®é˜¿é‡Œäº‘ç™¾ç‚¼å®¢æˆ·ç«¯
# è¯·æ›¿æ¢ä¸ºä½ çš„å®é™… API Key
API_KEY = os.getenv("DASHSCOPE_API_KEY")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

client = OpenAI(
    api_key=API_KEY,
    base_url=BASE_URL,
)

HISTORY_FILE = "chat_history.json"

def load_history():
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å†å²è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(messages):
    """ä¿å­˜å¯¹è¯åˆ°æœ¬åœ°"""
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=4)

# 2. å¯¹è¯é€»è¾‘
def predict(user_input, chatbot, system_prompt):
    if not user_input:
        yield "", chatbot
        return

    # æ„å»ºå‘é€ç»™ API çš„æ¶ˆæ¯åºåˆ—
    # chatbot æ ¼å¼ä¸º: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    api_messages = [{"role": "system", "content": system_prompt}]
    for msg in chatbot:
        api_messages.append(msg)

    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    current_user_msg = {"role": "user", "content": user_input}
    api_messages.append(current_user_msg)

    # æ›´æ–°ç•Œé¢ï¼šå…ˆæ˜¾ç¤ºç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶ä¸º AI å›å¤ç•™å‡ºå ä½
    chatbot.append(current_user_msg)
    chatbot.append({"role": "assistant", "content": ""})

    try:
        # ä½¿ç”¨ OpenAI SDK è¿›è¡Œæµå¼è°ƒç”¨
        completion = client.chat.completions.create(
            model="qwen-plus",  # æˆ–è€… qwen-turbo
            messages=api_messages,
            stream=True,
            stream_options={"include_usage": True}
        )

        full_response = ""
        for chunk in completion:
            # è¿‡æ»¤æ‰ usage ç­‰éå†…å®¹æ•°æ®å—
            if len(chunk.choices) > 0:
                content = chunk.choices[0].delta.content
                if content:
                    full_response += content
                    chatbot[-1]["content"] = full_response
                    yield "", chatbot  # å®æ—¶åˆ·æ–°ç•Œé¢

        # å¯¹è¯ç»“æŸåä¿å­˜
        save_history(chatbot)

    except Exception as e:
        chatbot[-1]["content"] = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
        yield "", chatbot

def clear_chat():
    if os.path.exists(HISTORY_FILE):
        os.remove(HISTORY_FILE)
    return [], ""

# 3. ç•Œé¢å¸ƒå±€
with gr.Blocks(title="é€šä¹‰åƒé—®åŠ©æ‰‹") as demo:
    gr.Markdown("""
    # ğŸ¤– é€šä¹‰åƒé—®æ™ºèƒ½åŠ©æ‰‹ (SDKç‰ˆ)
    **åŠŸèƒ½ï¼š** å¤šè½®å¯¹è¯ã€è‡ªå®šä¹‰äººè®¾ã€æœ¬åœ°è®°å½•ä¿å­˜ã€æµå¼å“åº”ã€‚
    """)

    with gr.Row():
        with gr.Column(scale=1):
            system_input = gr.Textbox(
                label="æœºå™¨äººäººè®¾è®¾å®š",
                value="ä½ æ˜¯ä¸€ä¸ªé€šæ™“å¤ä»Šã€è¯´è¯å¹½é»˜çš„ AI åŠ©æ‰‹ã€‚",
                lines=5
            )
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰å†å²", variant="stop")
            gr.Markdown("---")
            gr.Markdown("**æç¤ºï¼š** å†å²è®°å½•å°†è‡ªåŠ¨ä¿å­˜åœ¨åŒä¸€ç›®å½•ä¸‹çš„ `chat_history.json` ä¸­ã€‚")

        with gr.Column(scale=4):
            # æ˜¾å¼ä½¿ç”¨ type="messages" ä»¥åŒ¹é… OpenAI æ ¼å¼
            chatbot = gr.Chatbot(
                label="å¯¹è¯çª—å£",
                value=load_history(),
                height=550
            )
            with gr.Row():
                msg_input = gr.Textbox(
                    label="è¾“å…¥æ¡†",
                    placeholder="è¯·è¾“å…¥æ‚¨æƒ³é—®çš„é—®é¢˜ï¼ŒæŒ‰å›è½¦æäº¤...",
                    show_label=False,
                    scale=8
                )
                submit_btn = gr.Button("å‘é€", variant="primary", scale=1)

    # 4. äº‹ä»¶ç»‘å®š
    # æäº¤é€»è¾‘
    msg_input.submit(predict, [msg_input, chatbot, system_input], [msg_input, chatbot])
    submit_btn.click(predict, [msg_input, chatbot, system_input], [msg_input, chatbot])
    # æ¸…ç©ºé€»è¾‘
    clear_btn.click(clear_chat, None, [chatbot, msg_input])

if __name__ == "__main__":
    # å¯åŠ¨
    demo.queue().launch()
